import io
import re
import traceback
from fastapi import FastAPI, Request, UploadFile, File, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import os, json, httpx
import chardet
from langchain_community.document_loaders import PyPDFLoader
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel, Field
from langchain_core.output_parsers import JsonOutputParser
import uvicorn
from typing import List, Dict
from unstructured.partition.pdf import partition_pdf
import unicodedata
from hazm import Normalizer
from bs4 import BeautifulSoup
import random
import tempfile
from dotenv import load_dotenv

# âœ… ØªØºÛŒÛŒØ±: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Supabase Ø¨Ù‡ Ø¬Ø§ÛŒ SQLAlchemy
from db_config import supabase

load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
# ----------------------------
# FastAPI setup
# ----------------------------
app = FastAPI()

# âœ… ØªØºÛŒÛŒØ±: Ø­Ø°Ù startup event Ø¨Ø±Ø§ÛŒ SQLAlchemy
# Ø¯ÛŒÚ¯Ø± Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ create_all Ù†ÛŒØ³ØªØŒ Ø¬Ø¯ÙˆÙ„ Ø±Ø§ Ø¯Ø± Supabase Ø³Ø§Ø®ØªÛŒÙ…

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

MAX_CHUNK_SIZE = 1000
normalizer = Normalizer()
chat_memory = {}
MAX_MEMORY = 5
FALLBACK_SOURCES = {
    "contract": [
        {"title": "Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯Ù‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ø¨Ù„Ø§Ú©Ú†ÛŒÙ† Ø¯Ø± Ø§ÛŒØ±Ø§Ù†", "url": "https://networkerbash.ir/Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯Ù‡Ø§ÛŒ-Ù‡ÙˆØ´Ù…Ù†Ø¯-Ø¯Ø±-Ø§ÛŒØ±Ø§Ù†/",
         "text": ""},
        {"title": "Ø³Ø±Ù…Ø§ÛŒÙ‡ Ú¯Ø°Ø§Ø±ÛŒ Ù…Ù„Ú©ÛŒ Ø´Ù…Ø§Ù„ - Ø§Ø®Ø¨Ø§Ø± Ø´Ù…Ø§Ù„", "url": "https://www.shomalnews.com/", "text": ""},
        {"title": "Ù‚ÙˆØ§Ù†ÛŒÙ† Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯ Ø¯Ø± Ù‚Ø§Ù†ÙˆÙ† Ù…Ø¯Ù†ÛŒ Ø§ÛŒØ±Ø§Ù†", "url": "https://rc.majlis.ir/fa/law/show/99677", "text": ""},
        {"title": "ÙØ±ØµØªâ€ŒÙ‡Ø§ÛŒ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¯Ø± Ù…Ø§Ø²Ù†Ø¯Ø±Ø§Ù† Ùˆ Ú¯ÛŒÙ„Ø§Ù†", "url": "https://www.hamshahrionline.ir/tag/Ù…Ø§Ø²Ù†Ø¯Ø±Ø§Ù†",
         "text": ""},
        {"title": "Ø®Ø±ÛŒØ¯ Ùˆ ÙØ±ÙˆØ´ Ù…Ù„Ú© Ø¯Ø± Ø´Ù…Ø§Ù„ - Ø¯ÛŒÙˆØ§Ø±", "url": "https://seeone.net/Ø¨Ù„Ø§Ú¯/Ù†Ú©Ø§Øª-Ù…Ù‡Ù…-Ø¨Ø±Ø§ÛŒ-Ø®Ø±ÛŒØ¯-ÙˆÛŒÙ„Ø§-Ø¯Ø±-Ø´Ù…Ø§Ù„/",
         "text": ""},
        {"title": "ÙˆÛŒØ¯ÛŒÙˆÙ‡Ø§ÛŒ Ø³Ø±Ù…Ø§ÛŒÙ‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ù…Ù„Ú©ÛŒ - Ø¢Ù¾Ø§Ø±Ø§Øª",
         "url": "https://www.aparat.com/search/Ø³Ø±Ù…Ø§ÛŒÙ‡%20Ú¯Ø°Ø§Ø±ÛŒ%20Ù…Ù„Ú©ÛŒ%20Ø´Ù…Ø§Ù„", "text": ""},
        {"title": "Ø§Ø®Ø¨Ø§Ø± Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø´Ù…Ø§Ù„ Ú©Ø´ÙˆØ±", "url": "https://www.isna.ir/tag/Ø´Ù…Ø§Ù„", "text": ""},
        {"title": "Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯Ù‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ùˆ Ø¨Ù„Ø§Ú©Ú†ÛŒÙ† Ø¯Ø± Ø§ÛŒØ±Ø§Ù†", "url": "https://networkerbash.ir/Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯Ù‡Ø§ÛŒ-Ù‡ÙˆØ´Ù…Ù†Ø¯-Ø¯Ø±-Ø§ÛŒØ±Ø§Ù†/",
         "text": ""},
        {"title": "ØªÙˆØ±Ù… Ù…Ø³Ú©Ù† Ø¯Ø± Ø´Ù…Ø§Ù„ Ø§ÛŒØ±Ø§Ù†", "url": "https://www.eghtesadonline.com/tag/Ù…Ø³Ú©Ù†%20Ø´Ù…Ø§Ù„", "text": ""},
        {"title": "Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø®Ø±ÛŒØ¯ ÙˆÛŒÙ„Ø§ Ø¯Ø± Ø´Ù…Ø§Ù„", "url": "https://www.kojaro.com/pr/209705-buy-villa-north-pr/",
         "text": ""},
    ],
    "resume": [
        {"title": "Ù†Ù…ÙˆÙ†Ù‡ Ø±Ø²ÙˆÙ…Ù‡ ÙØ§Ø±Ø³ÛŒ", "url": "https://fa.wikipedia.org/wiki/Ø±Ø²ÙˆÙ…Ù‡", "text": ""},
        {"title": "Ø³Ø§Ø®Øª Ø±Ø²ÙˆÙ…Ù‡ Ø¢Ù†Ù„Ø§ÛŒÙ†", "url": "https://www.jobinja.ir/resume-builder", "text": ""},
    ],
    "will": [
        {"title": "ÙˆØµÛŒØªâ€ŒÙ†Ø§Ù…Ù‡ Ø¯Ø± Ù‚Ø§Ù†ÙˆÙ† Ø§ÛŒØ±Ø§Ù†", "url": "https://rc.majlis.ir/fa/law/show/99677", "text": ""},
    ],
    "default": [
        {"title": "ÙˆÛŒÚ©ÛŒâ€ŒÙ¾Ø¯ÛŒØ§ ÙØ§Ø±Ø³ÛŒ", "url": "https://fa.wikipedia.org", "text": ""},
        {"title": "Ø®Ø¨Ø±Ú¯Ø²Ø§Ø±ÛŒ Ø§ÛŒØ³Ù†Ø§", "url": "https://www.isna.ir", "text": ""},
        {"title": "Ø¯ÛŒÙˆØ§Ø± Ø§ÛŒØ±Ø§Ù†", "url": "https://divar.ir", "text": ""},
        {"title": "Ø¢Ù¾Ø§Ø±Ø§Øª", "url": "https://www.aparat.com", "text": ""},
        {"title": "Ù‡Ù…Ø´Ù‡Ø±ÛŒ Ø¢Ù†Ù„Ø§ÛŒÙ†", "url": "https://www.hamshahrionline.ir", "text": ""},
    ]
}


async def fetch_related_web_data(category: str, user_text: str) -> list:
    print("INFO: External search APIs blocked (sanctions). Using local fallback sources.")
    category_key = category.lower()
    sources_pool = FALLBACK_SOURCES.get(category_key, FALLBACK_SOURCES.get("default", []))
    if len(sources_pool) < 5:
        sources_pool += FALLBACK_SOURCES["default"]
    selected = random.sample(sources_pool, min(5, len(sources_pool)))
    print(
        f"DEBUG: Selected {len(selected)} fallback sources for category '{category}' from pool of {len(sources_pool)}")
    return selected


async def scrape_web_content(url: str) -> str:
    try:
        async with httpx.AsyncClient(
                timeout=25,
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                },
                follow_redirects=True
        ) as client:
            resp = await client.get(url)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")

            for script in soup(["script", "style"]):
                script.decompose()
            paragraphs = [p.get_text().strip() for p in soup.find_all("p")[:20] if p.get_text().strip()]
            text = "\n".join(paragraphs)[:4000]
            text = text.replace("\u200c", " ").strip()
            if text:
                print(f"DEBUG: Successfully scraped {len(text)} characters from {url}")
                return text
            else:
                print(f"WARNING: No text content scraped from {url}")
                return "No readable text found on this page."
    except Exception as e:
        print(f"ERROR scraping {url}: {str(e)}")
        return f"Failed to scrape this source: {str(e)}"


async def fetch_and_scrape_related(category: str, user_text: str) -> list:
    sources = await fetch_related_web_data(category, user_text)
    enriched_sources = []
    for src in sources:
        content = await scrape_web_content(src["url"])
        enriched_sources.append({
            "title": src["title"],
            "url": src["url"],
            "text": content
        })
    print(f"DEBUG: Completed scraping. Total enriched sources: {len(enriched_sources)}")
    if not enriched_sources:
        enriched_sources = [{
            "title": "Ù…Ù†Ø§Ø¨Ø¹ Ø®Ø§Ø±Ø¬ÛŒ Ù…Ø­Ø¯ÙˆØ¯ (ØªØ­Ø±ÛŒÙ…)",
            "url": "",
            "text": "Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒØŒ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯. Ù…Ù†Ø§Ø¨Ø¹ ÙˆØ¨ scrape Ù†Ø´Ø¯Ù†Ø¯."
        }]
    return enriched_sources


# ----------------------------
# LLM
# ----------------------------
async def github_llm(prompt: str) -> str:
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": "gpt-4.1-mini",
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.3,
        "stream": True
    }

    async with httpx.AsyncClient(timeout=None) as client:
        async with client.stream(
                "POST",
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=payload
        ) as response:

            if response.status_code != 200:
                raise Exception(f"OpenAI returned {response.status_code}")

            final_text = ""

            async for line in response.aiter_lines():
                if not line or not line.startswith("data:"):
                    continue

                raw = line.replace("data:", "").strip()

                if raw == "[DONE]":
                    break

                try:
                    data = json.loads(raw)
                except Exception:
                    continue

                choices = data.get("choices", [])
                if not choices:
                    continue

                delta = choices[0].get("delta", {})
                if not delta:
                    continue

                content = delta.get("content", "")
                if content:
                    final_text += content

            return final_text.strip()


# Pydantic models by category
class Contract(BaseModel):
    parties: List[str] = Field(description="Ø§Ø³Ø§Ù…ÛŒ Ø·Ø±ÙÛŒÙ† Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯")
    subject: str = Field(description="Ù…ÙˆØ¶ÙˆØ¹ Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯")
    duration: str = Field(description="Ù…Ø¯Øª Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯")
    conditions: List[str] = Field(description="Ø´Ø±Ø§ÛŒØ· Ùˆ ØªØ¹Ù‡Ø¯Ø§Øª")
    penalties: str = Field(description="Ø¬Ø±ÛŒÙ…Ù‡â€ŒÙ‡Ø§ Ùˆ Ø¶Ù…Ø§Ù†Øªâ€ŒÙ‡Ø§")
    signatures: List[str] = Field(description="Ø§Ù…Ø¶Ø§Ù‡Ø§")


class Resume(BaseModel):
    name: str = Field(description="Ù†Ø§Ù… Ùˆ Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ")
    contact: dict = Field(description="Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÙ…Ø§Ø³ (Ø§ÛŒÙ…ÛŒÙ„ØŒ ØªÙ„ÙÙ†)")
    education: List[dict] = Field(description="ØªØ­ØµÛŒÙ„Ø§Øª")
    experience: List[dict] = Field(description="ØªØ¬Ø±Ø¨ÛŒØ§Øª Ú©Ø§Ø±ÛŒ")
    skills: List[str] = Field(description="Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§")


class Will(BaseModel):
    testator: str = Field(description="Ù†Ø§Ù… ÙˆØµÛŒØªâ€ŒÚ©Ù†Ù†Ø¯Ù‡")
    beneficiaries: List[str] = Field(description="ÙˆØ§Ø±Ø«Ø§Ù† Ùˆ Ø°ÛŒâ€ŒÙ†ÙØ¹Ø§Ù†")
    assets: List[dict] = Field(description="Ø¯Ø§Ø±Ø§ÛŒÛŒâ€ŒÙ‡Ø§ Ùˆ Ù†Ø­ÙˆÙ‡ ØªÙ‚Ø³ÛŒÙ…")
    conditions: List[str] = Field(description="Ø´Ø±Ø§ÛŒØ· ÙˆØµÛŒØª")
    executor: str = Field(description="Ù…Ø¬Ø±ÛŒ ÙˆØµÛŒØª")


CATEGORY_MODELS = {
    "contract": Contract,
    "resume": Resume,
    "will": Will,
}


# select categories
@app.post("/select_category")
async def select_category(request: Request):
    body = await request.json()
    user_id = body.get("user_id")
    category = body.get("category")

    print("Raw category recive from front ", repr(category))
    if not user_id or not category:
        return JSONResponse(status_code=400, content={"error": "user_id and category are required"})

    # âœ… ØªØºÛŒÛŒØ±: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Supabase
    existing = supabase.table("ai_assist").select("*").eq("user_id", user_id).execute()

    if existing.data:
        # Ø¢Ù¾Ø¯ÛŒØª
        supabase.table("ai_assist").update({
            "category": category
        }).eq("user_id", user_id).execute()
    else:
        # Ø³Ø§Ø®Øª Ø¬Ø¯ÛŒØ¯
        supabase.table("ai_assist").insert({
            "user_id": user_id,
            "category": category,
            "data": {}
        }).execute()

    return {"message": f"Category '{category}' activated."}


# normalize persian text
def deep_clean_farsi_text(text: str) -> str:
    if not text:
        return ""
    text = unicodedata.normalize("NFKC", text)
    text = text.replace("ÙŠ", "ÛŒ").replace("Ùƒ", "Ú©")
    text = text.replace("â€Œ", " ").replace("\u200c", " ")
    text = normalizer.normalize(text)
    return text.strip()


def looks_garbled(text: str) -> bool:
    bad_patterns = [
        r"[Ø§Ø¢Ø¨Ù¾ØªØ«Ø¬Ú†Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚Ú©Ú¯Ù„Ù…Ù†ÙˆÙ‡ÛŒ]{1,2}\s[Ø§Ø¢Ø¨Ù¾ØªØ«Ø¬Ú†Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚Ú©Ú¯Ù„Ù…Ù†ÙˆÙ‡ÛŒ]{1,2}",
        r"[ï®ï»Ÿï»£ï»§ï»«ï»³ïºïºïº•ïº©ïº­ïº¯ïº±ïºµïº¹ïº¿ï»ï»…ï»‰ï»ï»‘ï»•ï»™ï»™ï»ï»¡ï»¥ï»©ï»±]",
    ]
    for pattern in bad_patterns:
        if re.search(pattern, text):
            return True
    return False


def chunk_text(text: str, size: int = MAX_CHUNK_SIZE) -> list[str]:
    return [text[i:i + size] for i in range(0, len(text), size)]


# pydantic models for scale data structure
class Person(BaseModel):
    id: str
    name: str
    source_ids: List[int] = Field(..., description="IDs Ø¨Ù„ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø¨Ø¹")


class Relation(BaseModel):
    from_id: str
    to_id: str
    type: str
    source_ids: List[int]


class FamilyTree(BaseModel):
    persons: List[Person]
    relations: List[Relation]
    other_data: Dict[str, str] = {}


@app.post("/upload_json")
async def upload_json(
        user_id: str = Form(...),
        category: str = Form(...),
        file: UploadFile = File(...)
):
    print("UPLOAD_JSON RECEIVED CATEGORY:", repr(category))
    content = await file.read()
    json_data = {}
    filename = file.filename.lower() if file.filename else ""

    try:
        if filename.endswith(".json"):
            json_data = json.loads(content.decode("utf-8", errors="ignore"))

        elif filename.endswith(".pdf"):
            print(f"ğŸ“„ Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ PDF: {file.filename}")
            print(f"ğŸ“¦ Ø­Ø¬Ù… ÙØ§ÛŒÙ„: {len(content)} Ø¨Ø§ÛŒØª")

            with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp_pdf:
                tmp_pdf.write(content)
                pdf_path = tmp_pdf.name
            print(f"ğŸ’¾ ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {pdf_path}")

            # Ù…Ø±Ø­Ù„Ù‡ 1: ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨Ø§ PyPDFLoader
            print("\nğŸ” Ù…Ø±Ø­Ù„Ù‡ 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨Ø§ PyPDFLoader...")
            context = ""
            context_blocks = []
            use_ocr = False

            try:
                reader = PyPDFLoader(pdf_path)
                pages = reader.load()
                print(f"âœ… ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª: {len(pages)}")

                for page_num, page in enumerate(pages, 1):
                    page_text = page.page_content
                    cleaned_text = deep_clean_farsi_text(page_text)

                    if cleaned_text:
                        print(f"   ğŸ“– ØµÙØ­Ù‡ {page_num}: {len(cleaned_text)} Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯")
                        print(f"   ğŸ“ Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´: {cleaned_text[:100]}...")
                        context += cleaned_text + "\n\n"
                        context_blocks.append({
                            "page": page_num,
                            "text": cleaned_text,
                            "char_count": len(cleaned_text)
                        })
                    else:
                        print(f"   âš ï¸  ØµÙØ­Ù‡ {page_num}: Ù…ØªÙ† Ù‚Ø§Ø¨Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")

                # Ø¨Ø±Ø±Ø³ÛŒ Ú©ÛŒÙÛŒØª Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡
                if looks_garbled(context):
                    print("âš ï¸  Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ù†Ø§Ù…ÙÙ‡ÙˆÙ… ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯ØŒ Ø³ÙˆØ¦ÛŒÚ† Ø¨Ù‡ OCR...")
                    use_ocr = True
                elif len(context.strip()) < 50:
                    print(f"âš ï¸  Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ Ø§Ø³Øª ({len(context.strip())} Ú©Ø§Ø±Ø§Ú©ØªØ±)ØŒ Ø³ÙˆØ¦ÛŒÚ† Ø¨Ù‡ OCR...")
                    use_ocr = True
                else:
                    print(f"âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ù…ÙˆÙÙ‚: Ú©Ù„ {len(context)} Ú©Ø§Ø±Ø§Ú©ØªØ±")

            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± PyPDFLoader: {str(e)}")
                print(f"   Ø³ÙˆØ¦ÛŒÚ† Ø¨Ù‡ OCR...")
                use_ocr = True

            # Ù…Ø±Ø­Ù„Ù‡ 2: Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² OCR
            if use_ocr:
                print("\nğŸ” Ù…Ø±Ø­Ù„Ù‡ 2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨Ø§ OCR (partition_pdf)...")
                try:
                    elements = partition_pdf(pdf_path, strategy="hi_res", languages=["fas"])
                    print(f"âœ… ØªØ¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {len(elements)}")

                    context_blocks = []
                    full_text = []

                    for i, el in enumerate(elements):
                        text = deep_clean_farsi_text(el.text) if hasattr(el, 'text') else ""
                        if text:
                            block = {
                                "id": i,
                                "type": el.category if hasattr(el, 'category') else "unknown",
                                "text": text,
                                "char_count": len(text)
                            }

                            if hasattr(el, 'bbox'):
                                block["bbox"] = el.bbox

                            print(f"   ğŸ“¦ Ø¨Ù„ÙˆÚ© {i} ({block['type']}): {len(text)} Ú©Ø§Ø±Ø§Ú©ØªØ±")
                            print(f"      {text[:80]}...")

                            context_blocks.append(block)
                            full_text.append(text)

                    context = "\n\n".join(full_text)
                    print(f"âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ OCR Ù…ÙˆÙÙ‚: Ú©Ù„ {len(context)} Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§Ø² {len(context_blocks)} Ø¨Ù„ÙˆÚ©")

                except Exception as e:
                    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± OCR: {str(e)}")
                    traceback.print_exc()
                    raise Exception(f"Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ù†Ø§Ù…ÙˆÙÙ‚: {str(e)}")

            # Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª
            try:
                os.unlink(pdf_path)
                print(f"ğŸ—‘ï¸  ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª Ø­Ø°Ù Ø´Ø¯")
            except Exception as e:
                print(f"âš ï¸  Ø®Ø·Ø§ Ø¯Ø± Ø­Ø°Ù ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª: {str(e)}")

            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ JSON Ù†Ù‡Ø§ÛŒÛŒ
            print("\nğŸ“Š Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ JSON Ù†Ù‡Ø§ÛŒÛŒ...")
            json_data = {
                "filename": file.filename,
                "category": category.strip().lower(),
                "extraction_method": "ocr" if use_ocr else "text",
                "total_characters": len(context),
                "total_blocks": len(context_blocks),
                "full_text": context,
                "blocks": context_blocks,
                "metadata": {
                    "processed_at": str(json.dumps(context_blocks, ensure_ascii=False)),
                    "file_size_bytes": len(content)
                }
            }

            print(f"âœ… JSON Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯:")
            print(f"   - Ø±ÙˆØ´ Ø§Ø³ØªØ®Ø±Ø§Ø¬: {json_data['extraction_method']}")
            print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø§Ú©ØªØ±: {json_data['total_characters']}")
            print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ø¨Ù„ÙˆÚ©: {json_data['total_blocks']}")
            print(f"   - Ù¾ÛŒØ´â€ŒÙ†Ù…Ø§ÛŒØ´ Ù…ØªÙ† Ú©Ø§Ù…Ù„:\n{context[:500]}...")


        elif filename.endswith(".txt"):
            detected = chardet.detect(content)
            encoding = detected.get("encoding") or "utf-8"
            raw_text = content.decode(encoding, errors="ignore")
            json_data = {"text": deep_clean_farsi_text(raw_text)}

        elif filename.endswith(".docx"):
            from docx import Document
            doc = Document(io.BytesIO(content))
            full_text = "\n".join([para.text for para in doc.paragraphs])
            cleaned = deep_clean_farsi_text(full_text)
            json_data = {"text": cleaned}

        else:
            return JSONResponse(
                status_code=400,
                content={"error": "Unsupported file type. Only JSON, PDF, TXT."}
            )

        # âœ… Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Supabase
        print(f"\nğŸ’¾ Ø´Ø±ÙˆØ¹ Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø± Supabase...")
        print(f"   - User ID: {user_id}")
        print(f"   - Category: {category}")
        print(f"   - Ø­Ø¬Ù… JSON: {len(json.dumps(json_data, ensure_ascii=False))} Ú©Ø§Ø±Ø§Ú©ØªØ±")

        # Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù…Ù†Ø§Ø¨Ø¹ ÙˆØ¨ Ù‡Ø³Øª (ÙØ¹Ù„Ø§Ù‹ ØºÛŒØ±ÙØ¹Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…)
        related_data = []
        # text_data = json.dumps(json_data, ensure_ascii=False)
        # related_data = await fetch_and_scrape_related(category, text_data)

        existing = supabase.table("ai_assist").select("*").eq("user_id", user_id).execute()
        print(f"   - Ø¨Ø±Ø±Ø³ÛŒ Ø±Ú©ÙˆØ±Ø¯ Ù…ÙˆØ¬ÙˆØ¯: {'ÛŒØ§ÙØª Ø´Ø¯' if existing.data else 'ÛŒØ§ÙØª Ù†Ø´Ø¯'}")

        if existing.data:
            print(f"   - Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø±Ú©ÙˆØ±Ø¯ Ù…ÙˆØ¬ÙˆØ¯...")
            result = supabase.table("ai_assist").update({
                "category": category,
                "data": json_data,
                "related_sources": related_data
            }).eq("user_id", user_id).execute()
            print(f"   âœ… Ø±Ú©ÙˆØ±Ø¯ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯")
        else:
            print(f"   - Ø³Ø§Ø®Øª Ø±Ú©ÙˆØ±Ø¯ Ø¬Ø¯ÛŒØ¯...")
            result = supabase.table("ai_assist").insert({
                "user_id": user_id,
                "category": category,
                "data": json_data,
                "related_sources": related_data
            }).execute()
            print(f"   âœ… Ø±Ú©ÙˆØ±Ø¯ Ø¬Ø¯ÛŒØ¯ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯")

        print(f"âœ… Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø± Supabase Ù…ÙˆÙÙ‚")
        print(f"\nğŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ù¾Ø±Ø¯Ø§Ø²Ø´:")
        print(f"   - Ù†Ø§Ù… ÙØ§ÛŒÙ„: {file.filename}")
        print(f"   - Ù†ÙˆØ¹ ÙØ§ÛŒÙ„: {filename.split('.')[-1]}")
        print(f"   - Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ: {category}")
        print(f"   - ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {json_data.get('total_characters', len(str(json_data)))}")

        return {
            "message": f"File '{file.filename}' processed successfully âœ…",
            "category": category,
            "file_type": filename.split('.')[-1],
            "extraction_summary": {
                "method": json_data.get("extraction_method", "unknown"),
                "total_characters": json_data.get("total_characters", 0),
                "total_blocks": json_data.get("total_blocks", 0)
            },
            "json_data_preview": json.dumps(json_data, ensure_ascii=False)[:500],
        }

    except Exception as e:
        print(f"ERROR in upload_json: {str(e)}")
        traceback.print_exc()
        return JSONResponse(status_code=500, content={"error": f"Processing failed: {str(e)}"})


# asking method
@app.post("/ask")
async def ask(request: Request):
    body = await request.json()
    user_id = body.get("user_id")
    question = body.get("question")

    if not user_id or not question:
        return JSONResponse(status_code=400, content={"error": "user_id and question required."})

    # âœ… ØªØºÛŒÛŒØ±: Ú¯Ø±ÙØªÙ† Ø¯Ø§Ø¯Ù‡ Ø§Ø² Supabase
    result = supabase.table("ai_assist").select("*").eq("user_id", user_id).execute()

    if not result.data:
        return JSONResponse(status_code=400, content={"error": "No data for this user."})

    record = result.data[0]
    formatted_data = json.dumps(record["data"], ensure_ascii=False, indent=2)

    web_sources = ""
    if record.get("related_sources"):
        web_sources = "\n Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø±ØªØ¨Ø· Ø§Ø² ÙˆØ¨:\n"
        for idx, source in enumerate(record["related_sources"][:5], 1):
            web_sources += f"\n{idx}. {source.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')}\n"
            if source.get('text'):
                preview = source['text'][:500] + "..." if len(source['text']) > 500 else source['text']
                web_sources += f" Ù…Ø­ØªÙˆØ§: {preview}\n"

    history = chat_memory.get(user_id, [])

    conversation_context = ""
    if history:
        conversation_context = "\n".join(
            [f"{msg['role']}: {msg['content']}" for msg in history]
        )

    prompt = f"""
    ØªÙˆ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒ Ú©Ù‡ Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ø§ Ø¯Ù‚ØªØŒ Ù…Ù†Ø·Ù‚ Ùˆ Ù„Ø­Ù† Ø·Ø¨ÛŒØ¹ÛŒ Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯ÛŒ.
    Ù‡Ø¯Ù ØªÙˆ Ø§ÛŒÙ†Ù‡ Ú©Ù‡ Ú©Ø§Ø±Ø¨Ø± Ø­Ø³ Ú©Ù†Ù‡ Ø¨Ø§ ÛŒÙ‡ Ù…ØªØ®ØµØµ ØµÙ…ÛŒÙ…ÛŒ Ùˆ Ø¨Ø§ØªØ¬Ø±Ø¨Ù‡ Ø¯Ø± Ø­Ø§Ù„ Ú¯ÙØªâ€ŒÙˆÚ¯ÙˆØ¦Ù‡.

    ğŸ“‚ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ: {record["category"]}
    ğŸ“‹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:
    {formatted_data}
    {web_sources}

    ğŸ’¬ Ø­Ø§ÙØ¸Ù‡ Ú¯ÙØªÚ¯Ùˆ ØªØ§ Ø§ÛŒÙ† Ù„Ø­Ø¸Ù‡:
    {conversation_context}

    â“ Ø³Ø¤Ø§Ù„ Ø¬Ø¯ÛŒØ¯ Ú©Ø§Ø±Ø¨Ø±:
    {question}

    ğŸ“˜ Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ Ù¾Ø§Ø³Ø®â€ŒÚ¯ÙˆÛŒÛŒ:

    1. **Ù…Ø±Ø­Ù„Ù‡ Ø§ÙˆÙ„ â€” Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§**
       - Ø§Ø¨ØªØ¯Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª JSON Ùˆ Ù…Ù†Ø§Ø¨Ø¹ ÙˆØ¨ Ø¯Ø§Ø®Ù„ÛŒ Ø±Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†.
       - Ø±ÙˆØ§Ø¨Ø· Ø¨ÛŒÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ø§Ø´Ø®Ø§Øµ Ø±Ùˆ ØªØ­Ù„ÛŒÙ„ Ú©Ù†.
       - Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ù…Ø³ØªÙ‚ÛŒÙ… Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯ÛŒØŒ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‡Ù…ÙˆÙ† ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯Ù‡.
       - Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ø¨Ù†ÙˆÛŒØ³: Â«Ù…Ù†Ø¨Ø¹: Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§ÛŒÙ„Â» ÛŒØ§ Â«Ù…Ù†Ø¨Ø¹: ÙˆØ¨ Ø¯Ø§Ø®Ù„ÛŒÂ».

    2. **Ù…Ø±Ø­Ù„Ù‡ Ø¯ÙˆÙ… â€” Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯ Ù¾Ø§Ø³Ø® ØµØ±ÛŒØ­**
       - Ø§Ú¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ù¾Ø§Ø³Ø® Ø¯Ù‚ÛŒÙ‚ÛŒ Ù†Ø¯Ø§Ø¯Ù†ØŒ Ø§Ø² Ø¯Ø§Ù†Ø´ Ú©Ù„ÛŒ ÛŒØ§ Ø¬Ø³ØªØ¬ÙˆÛŒ ÙˆØ¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†.
       - Ù¾Ø§Ø³Ø® Ø±Ùˆ Ø®Ù„Ø§ØµÙ‡ØŒ Ø´ÙØ§Ù Ùˆ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø¨Ù†ÙˆÛŒØ³ (Ø­Ø¯ÙˆØ¯ Û² ØªØ§ Ûµ Ø¬Ù…Ù„Ù‡).
       - Ù¾Ø§Ø³Ø® Ø±Ùˆ Ø¨Ø§ Ø¹Ø¨Ø§Ø±Øª Â«ğŸ” Ø±ÙØªÙ… Ø³Ø±Ú† Ú©Ø±Ø¯Ù… Ùˆ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù… Ú©Ù‡...Â» Ø´Ø±ÙˆØ¹ Ú©Ù†.
       - Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ Ù…Ù†Ø¨Ø¹ ÙˆØ¨ Ø±Ùˆ Ø°Ú©Ø± Ú©Ù†.

    3. **Ù†Ú©Ø§Øª Ù„Ø­Ù† Ùˆ Ø¨ÛŒØ§Ù†**
       - Ù…Ø­ØªØ±Ù…Ø§Ù†Ù‡ØŒ Ø·Ø¨ÛŒØ¹ÛŒ Ùˆ ØµÙ…ÛŒÙ…ÛŒ Ø¨Ù†ÙˆÛŒØ³.
       -Ø­Ø§Ù„Ø§ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡:
"""

    answer = await github_llm(prompt)

    history.append({"role": "user", "content": question})
    history.append({"role": "assistant", "content": answer})
    chat_memory[user_id] = history[-MAX_MEMORY:]

    return {"answer": answer}


# endpoint Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª JSON Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡
@app.get("/get_extracted_data/{user_id}")
async def get_extracted_data(user_id: str):
    """
    Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ JSON Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú©Ø§Ø±Ø¨Ø±
    """
    print(f"\nğŸ” Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ user_id: {user_id}")

    try:
        result = supabase.table("ai_assist").select("*").eq("user_id", user_id).execute()

        if not result.data:
            print(f"âŒ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ user_id={user_id} ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return JSONResponse(
                status_code=404,
                content={"error": f"No data found for user_id: {user_id}"}
            )

        record = result.data[0]
        print(f"âœ… Ø¯Ø§Ø¯Ù‡ ÛŒØ§ÙØª Ø´Ø¯:")
        print(f"   - Category: {record.get('category')}")
        print(f"   - Data keys: {list(record.get('data', {}).keys())}")

        return {
            "user_id": user_id,
            "category": record.get("category"),
            "data": record.get("data"),
            "related_sources": record.get("related_sources", [])
        }

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡: {str(e)}")
        traceback.print_exc()
        return JSONResponse(
            status_code=500,
            content={"error": f"Failed to retrieve data: {str(e)}"}
        )


if __name__ == "__main__":
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        timeout_keep_alive=120,
        limit_concurrency=50,
        limit_max_requests=500
    )