import io
import re
import traceback
from fastapi import FastAPI, Request, UploadFile, File, Form
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import os, json, httpx
import chardet
from langchain_community.document_loaders import PyPDFLoader
from pydantic import BaseModel, Field
import uvicorn
from typing import List, Dict
import unicodedata
from hazm import Normalizer
from bs4 import BeautifulSoup
import random
import tempfile
from dotenv import load_dotenv
# âœ… ØªØºÛŒÛŒØ±: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Supabase Ø¨Ù‡ Ø¬Ø§ÛŒ SQLAlchemy
from db_config import supabase
# Ø§ÙØ²ÙˆØ¯Ù† Ø§ÛŒÙ…Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Azure AI Inference
from azure.ai.inference import ChatCompletionsClient
from azure.core.credentials import AzureKeyCredential
from azure.ai.inference.models import UserMessage

load_dotenv()
print("SUPABASE_KEY:", repr(os.getenv("SUPABASE_SERVICE_KEY")))

# ØªØºÛŒÛŒØ±: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² GITHUB_TOKEN Ø¨Ù‡ Ø¬Ø§ÛŒ OPENAI_API_KEY
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
ENDPOINT = "https://models.inference.ai.azure.com"
MODEL_NAME = "gpt-4o"  # ØªØºÛŒÛŒØ± Ø§Ø² "gpt-4.1-mini" Ø¨Ù‡ "gpt-4o-mini" Ú©Ù‡ Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª

# ----------------------------
# FastAPI setup
# ----------------------------
app = FastAPI()

# âœ… ØªØºÛŒÛŒØ±: Ø­Ø°Ù startup event Ø¨Ø±Ø§ÛŒ SQLAlchemy # Ø¯ÛŒÚ¯Ø± Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ create_all Ù†ÛŒØ³ØªØŒ Ø¬Ø¯ÙˆÙ„ Ø±Ø§ Ø¯Ø± Supabase Ø³Ø§Ø®ØªÛŒÙ…

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

MAX_CHUNK_SIZE = 1000
normalizer = Normalizer()
chat_memory = {}
MAX_MEMORY = 5





async def scrape_web_content(url: str) -> str:
    try:
        async with httpx.AsyncClient(
                timeout=25,
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                },
                follow_redirects=True
        ) as client:
            resp = await client.get(url)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")
            for script in soup(["script", "style"]):
                script.decompose()
            paragraphs = [p.get_text().strip() for p in soup.find_all("p")[:20] if p.get_text().strip()]
            text = "\n".join(paragraphs)[:4000]
            text = text.replace("\u200c", " ").strip()
            if text:
                print(f"DEBUG: Successfully scraped {len(text)} characters from {url}")
                return text
            else:
                print(f"WARNING: No text content scraped from {url}")
                return "No readable text found on this page."
    except Exception as e:
        print(f"ERROR scraping {url}: {str(e)}")
        return f"Failed to scrape this source: {str(e)}"





# ----------------------------
# ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø·ÙˆÙ„ Ù…ØªÙ†
# ----------------------------
def truncate_text(text: str, max_chars: int = 3000) -> str:
    """Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø§Ú©ØªØ± Ù…Ø´Ø®Øµ"""
    if len(text) <= max_chars:
        return text
    return text[:max_chars] + "... [Ø§Ø¯Ø§Ù…Ù‡ Ù…ØªÙ† Ø­Ø°Ù Ø´Ø¯]"


def estimate_tokens(text: str) -> int:
    """ØªØ®Ù…ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ (ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ 1 ØªÙˆÚ©Ù† = 4 Ú©Ø§Ø±Ø§Ú©ØªØ± Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ)"""
    return len(text) // 4


# ----------------------------
# LLM
# ----------------------------
async def github_llm(prompt: str) -> str:
    # Ø¨Ø±Ø±Ø³ÛŒ Ø·ÙˆÙ„ Ù¾Ø±Ø§Ù…Ù¾Øª Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø±Ø³Ø§Ù„
    estimated_tokens = estimate_tokens(prompt)
    print(f"ğŸ“Š ØªØ®Ù…ÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Ø§Ù…Ù¾Øª: {estimated_tokens}")

    if estimated_tokens > 7000:  # Ø­Ø¯ Ø§Ù…Ù† Ú©Ù…ØªØ± Ø§Ø² 8000
        print(f"âš ï¸ Ù¾Ø±Ø§Ù…Ù¾Øª Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ Ø§Ø³Øª ({estimated_tokens} ØªÙˆÚ©Ù†). Ø¯Ø± Ø­Ø§Ù„ Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù†...")
        # Ø§Ú¯Ø± Ù¾Ø±Ø§Ù…Ù¾Øª Ø®ÛŒÙ„ÛŒ Ø¨Ø²Ø±Ú¯ Ø¨ÙˆØ¯ØŒ Ø¢Ù† Ø±Ø§ Ú©ÙˆØªØ§Ù‡ Ú©Ù†
        prompt = prompt[:28000]  # Ø­Ø¯ÙˆØ¯ 7000 ØªÙˆÚ©Ù†

    client = ChatCompletionsClient(
        endpoint=ENDPOINT,
        credential=AzureKeyCredential(GITHUB_TOKEN)
    )

    final_text = ""

    try:
        # Ø§Ú¯Ø± complete async Ø¨Ø§Ø´Ù‡
        maybe_async = client.complete(
            stream=True,
            messages=[UserMessage(content=prompt)],
            model=MODEL_NAME,
            temperature=0.3
        )

        if hasattr(maybe_async, "__aiter__"):
            # async iterator
            async for update in maybe_async:
                if update.choices and update.choices[0].delta and update.choices[0].delta.content:
                    final_text += update.choices[0].delta.content
        else:
            # sync iterator
            for update in maybe_async:
                if update.choices and update.choices[0].delta and update.choices[0].delta.content:
                    final_text += update.choices[0].delta.content

    except Exception as e:
        raise Exception(f"Azure AI Inference returned error: {str(e)}")
    finally:
        # ÙÙ‚Ø· Ø§Ú¯Ø± close async Ù‡Ø³Øª await Ú©Ù†
        close_fn = getattr(client, "close", None)
        if close_fn:
            if callable(close_fn):
                maybe_awaitable = close_fn()
                if hasattr(maybe_awaitable, "__await__"):
                    await maybe_awaitable  # safe await

    return final_text.strip()


# Pydantic models by category
class Contract(BaseModel):
    parties: List[str] = Field(description="Ø§Ø³Ø§Ù…ÛŒ Ø·Ø±ÙÛŒÙ† Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯")
    subject: str = Field(description="Ù…ÙˆØ¶ÙˆØ¹ Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯")
    duration: str = Field(description="Ù…Ø¯Øª Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯")
    conditions: List[str] = Field(description="Ø´Ø±Ø§ÛŒØ· Ùˆ ØªØ¹Ù‡Ø¯Ø§Øª")
    penalties: str = Field(description="Ø¬Ø±ÛŒÙ…Ù‡â€ŒÙ‡Ø§ Ùˆ Ø¶Ù…Ø§Ù†Øªâ€ŒÙ‡Ø§")
    signatures: List[str] = Field(description="Ø§Ù…Ø¶Ø§Ù‡Ø§")


class Resume(BaseModel):
    name: str = Field(description="Ù†Ø§Ù… Ùˆ Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ")
    contact: dict = Field(description="Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÙ…Ø§Ø³ (Ø§ÛŒÙ…ÛŒÙ„ØŒ ØªÙ„ÙÙ†)")
    education: List[dict] = Field(description="ØªØ­ØµÛŒÙ„Ø§Øª")
    experience: List[dict] = Field(description="ØªØ¬Ø±Ø¨ÛŒØ§Øª Ú©Ø§Ø±ÛŒ")
    skills: List[str] = Field(description="Ù…Ù‡Ø§Ø±Øªâ€ŒÙ‡Ø§")


class Will(BaseModel):
    testator: str = Field(description="Ù†Ø§Ù… ÙˆØµÛŒØªâ€ŒÚ©Ù†Ù†Ø¯Ù‡")
    beneficiaries: List[str] = Field(description="ÙˆØ§Ø±Ø«Ø§Ù† Ùˆ Ø°ÛŒâ€ŒÙ†ÙØ¹Ø§Ù†")
    assets: List[dict] = Field(description="Ø¯Ø§Ø±Ø§ÛŒÛŒâ€ŒÙ‡Ø§ Ùˆ Ù†Ø­ÙˆÙ‡ ØªÙ‚Ø³ÛŒÙ…")
    conditions: List[str] = Field(description="Ø´Ø±Ø§ÛŒØ· ÙˆØµÛŒØª")
    executor: str = Field(description="Ù…Ø¬Ø±ÛŒ ÙˆØµÛŒØª")


CATEGORY_MODELS = {
    "contract": Contract,
    "resume": Resume,
    "will": Will,
}


# select categories
@app.post("/select_category")
async def select_category(request: Request):
    body = await request.json()
    user_id = body.get("user_id")
    category = body.get("category")
    print("Raw category recive from front ", repr(category))
    if not user_id or not category:
        return JSONResponse(status_code=400, content={"error": "user_id and category are required"})

    # âœ… ØªØºÛŒÛŒØ±: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Supabase
    existing = supabase.table("ai_assist").select("*").eq("user_id", user_id).execute()
    if existing.data:
        # Ø¢Ù¾Ø¯ÛŒØª
        supabase.table("ai_assist").update({
            "category": category
        }).eq("user_id", user_id).execute()
    else:
        # Ø³Ø§Ø®Øª Ø¬Ø¯ÛŒØ¯
        supabase.table("ai_assist").insert({
            "user_id": user_id,
            "category": category,
            "data": {}
        }).execute()

    return {"message": f"Category '{category}' activated."}


# normalize persian text
def deep_clean_farsi_text(text: str) -> str:
    if not text:
        return ""
    text = unicodedata.normalize("NFKC", text)
    text = text.replace("ÙŠ", "ÛŒ").replace("Ùƒ", "Ú©")
    text = text.replace("â€Œ", " ").replace("\u200c", " ")
    text = normalizer.normalize(text)
    return text.strip()


def looks_garbled(text: str) -> bool:
    bad_patterns = [
        r"[Ø§Ø¢Ø¨Ù¾ØªØ«Ø¬Ú†Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚Ú©Ú¯Ù„Ù…Ù†ÙˆÙ‡ÛŒ]{1,2}\s[Ø§Ø¢Ø¨Ù¾ØªØ«Ø¬Ú†Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚Ú©Ú¯Ù„Ù…Ù†ÙˆÙ‡ÛŒ]{1,2}",
        r"[ï®ï»Ÿï»£ï»§ï»«ï»³ïºïºïº•ïº©ïº­ïº¯ïº±ïºµïº¹ïº¿ï»ï»…ï»‰ï»ï»‘ï»•ï»™ï»™ï»ï»¡ï»¥ï»©ï»±]",
    ]
    for pattern in bad_patterns:
        if re.search(pattern, text):
            return True
    return False


def chunk_text(text: str, size: int = MAX_CHUNK_SIZE) -> list[str]:
    return [text[i:i + size] for i in range(0, len(text), size)]


# pydantic models for scale data structure
class Person(BaseModel):
    id: str
    name: str
    source_ids: List[int] = Field(..., description="IDs Ø¨Ù„ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø¨Ø¹")


class Relation(BaseModel):
    from_id: str
    to_id: str
    type: str
    source_ids: List[int]


class FamilyTree(BaseModel):
    persons: List[Person]
    relations: List[Relation]
    other_data: Dict[str, str] = {}


@app.post("/upload_json")
async def upload_json(
        user_id: str = Form(...),
        category: str = Form(...),
        file: UploadFile = File(...)
):
    print("UPLOAD_JSON RECEIVED CATEGORY:", repr(category))
    content = await file.read()
    json_data = {}
    filename = file.filename.lower() if file.filename else ""
    try:
        if filename.endswith(".json"):
            json_data = json.loads(content.decode("utf-8", errors="ignore"))

        elif filename.endswith(".pdf"):
            print(f"ğŸ“„ Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ PDF: {file.filename}")
            print(f"ğŸ“¦ Ø­Ø¬Ù… ÙØ§ÛŒÙ„: {len(content)} Ø¨Ø§ÛŒØª")
            with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp_pdf:
                tmp_pdf.write(content)
                pdf_path = tmp_pdf.name
            print(f"ğŸ’¾ ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {pdf_path}")

            context = ""
            context_blocks = []
            use_ocr = False

            # Ù…Ø±Ø­Ù„Ù‡ 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨Ø§ PyPDFLoader
            try:
                reader = PyPDFLoader(pdf_path)
                pages = reader.load()
                print(f"âœ… ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª: {len(pages)}")
                for page_num, page in enumerate(pages, 1):
                    page_text = page.page_content
                    cleaned_text = deep_clean_farsi_text(page_text)
                    if cleaned_text:
                        context += cleaned_text + "\n\n"
                        context_blocks.append({
                            "page": page_num,
                            "text": cleaned_text,
                            "char_count": len(cleaned_text)
                        })
                if len(context.strip()) < 50 or looks_garbled(context):
                    use_ocr = True
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± PyPDFLoader: {str(e)}")
                use_ocr = True

            # Ù…Ø±Ø­Ù„Ù‡ 2: Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨Ù‡ OCR Ø¨ÙˆØ¯ØŒ Ø§Ø² PyMuPDF Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            if use_ocr:
                print("ğŸ” Ù†ÛŒØ§Ø² Ø¨Ù‡ OCR ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯...")
                try:
                    # ØªÙ„Ø§Ø´ Ø¨Ø§ PyMuPDF Ø§Ú¯Ø± Ù†ØµØ¨ Ø¨Ø§Ø´Ø¯
                    try:
                        import fitz  # PyMuPDF
                        print("Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² PyMuPDF Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ†...")
                        doc = fitz.open(pdf_path)
                        for page_num in range(len(doc)):
                            page = doc.load_page(page_num)
                            text = page.get_text()
                            if text:
                                cleaned_text = deep_clean_farsi_text(text)
                                context += cleaned_text + "\n\n"
                                context_blocks.append({
                                    "page": page_num + 1,
                                    "text": cleaned_text,
                                    "char_count": len(cleaned_text),
                                    "method": "pymupdf"
                                })
                        doc.close()
                    except ImportError:
                        print("PyMuPDF Ù†ØµØ¨ Ù†ÛŒØ³ØªØŒ Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø§ Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡...")
                except Exception as e:
                    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ OCR: {str(e)}")
                    traceback.print_exc()

            # Ø­Ø°Ù ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª
            try:
                os.unlink(pdf_path)
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø­Ø°Ù ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª: {str(e)}")

            json_data = {
                "filename": file.filename,
                "category": category.strip().lower(),
                "extraction_method": "ocr" if use_ocr else "text",
                "total_characters": len(context),
                "total_blocks": len(context_blocks),
                "full_text": context,
                "blocks": context_blocks,
                "metadata": {
                    "processed_at": str(json.dumps(context_blocks, ensure_ascii=False)),
                    "file_size_bytes": len(content)
                }
            }

        elif filename.endswith(".txt"):
            detected = chardet.detect(content)
            encoding = detected.get("encoding") or "utf-8"
            raw_text = content.decode(encoding, errors="ignore")
            json_data = {"text": deep_clean_farsi_text(raw_text)}

        elif filename.endswith(".docx"):
            from docx import Document
            doc = Document(io.BytesIO(content))
            full_text = "\n".join([para.text for para in doc.paragraphs])
            json_data = {"text": deep_clean_farsi_text(full_text)}

        else:
            return JSONResponse(
                status_code=400,
                content={"error": "Unsupported file type. Only JSON, PDF, TXT, DOCX."}
            )

        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Supabase
        related_data = []
        existing = supabase.table("ai_assist").select("*").eq("user_id", user_id).execute()
        if existing.data:
            supabase.table("ai_assist").update({
                "category": category,
                "data": json_data,
                "related_sources": related_data
            }).eq("user_id", user_id).execute()
        else:
            supabase.table("ai_assist").insert({
                "user_id": user_id,
                "category": category,
                "data": json_data,
                "related_sources": related_data
            }).execute()

        return {
            "message": f"File '{file.filename}' processed successfully âœ…",
            "category": category,
            "file_type": filename.split('.')[-1],
            "extraction_summary": {
                "method": json_data.get("extraction_method", "unknown"),
                "total_characters": json_data.get("total_characters", 0),
                "total_blocks": json_data.get("total_blocks", 0)
            },
            "json_data_preview": json.dumps(json_data, ensure_ascii=False)[:500],
        }

    except Exception as e:
        print(f"ERROR in upload_json: {str(e)}")
        traceback.print_exc()
        return JSONResponse(status_code=500, content={"error": f"Processing failed: {str(e)}"})


# asking method
@app.post("/ask")
async def ask(request: Request):
    body = await request.json()
    user_id = body.get("user_id")
    question = body.get("question")
    if not user_id or not question:
        return JSONResponse(status_code=400, content={"error": "user_id and question required."})

    # âœ… ØªØºÛŒÛŒØ±: Ú¯Ø±ÙØªÙ† Ø¯Ø§Ø¯Ù‡ Ø§Ø² Supabase
    result = supabase.table("ai_assist").select("*").eq("user_id", user_id).execute()
    if not result.data:
        return JSONResponse(status_code=400, content={"error": "No data for this user."})

    record = result.data[0]

    # âœ… Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ÛŒ token limit
    data_to_format = record["data"]

    # Ø§Ú¯Ø± full_text ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ø¢Ù† Ø±Ø§ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù†
    if isinstance(data_to_format, dict) and "full_text" in data_to_format:
        data_to_format = data_to_format.copy()
        data_to_format["full_text"] = truncate_text(data_to_format["full_text"], max_chars=2000)

    # Ø§Ú¯Ø± blocks ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ ØªØ¹Ø¯Ø§Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù†
    if isinstance(data_to_format, dict) and "blocks" in data_to_format:
        data_to_format["blocks"] = data_to_format["blocks"][:5]  # ÙÙ‚Ø· 5 Ø¨Ù„ÙˆÚ© Ø§ÙˆÙ„

    formatted_data = json.dumps(data_to_format, ensure_ascii=False, indent=2)
    formatted_data = truncate_text(formatted_data, max_chars=3000)

    # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ù…Ù†Ø§Ø¨Ø¹ ÙˆØ¨
    web_sources = ""
    if record.get("related_sources"):
        web_sources = "\n Ù…Ù†Ø§Ø¨Ø¹ Ù…Ø±ØªØ¨Ø· Ø§Ø² ÙˆØ¨:\n"
        for idx, source in enumerate(record["related_sources"][:3], 1):  # ÙÙ‚Ø· 3 Ù…Ù†Ø¨Ø¹ Ø§ÙˆÙ„
            web_sources += f"\n{idx}. {source.get('title', 'Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†')}\n"
            if source.get('text'):
                preview = truncate_text(source['text'], max_chars=300)
                web_sources += f" Ù…Ø­ØªÙˆØ§: {preview}\n"

    history = chat_memory.get(user_id, [])
    conversation_context = ""
    if history:
        # ÙÙ‚Ø· 3 Ù¾ÛŒØ§Ù… Ø¢Ø®Ø± Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±
        recent_history = history[-3:]
        conversation_context = "\n".join(
            [f"{msg['role']}: {truncate_text(msg['content'], max_chars=200)}" for msg in recent_history]
        )

    prompt = f"""
    ØªÙˆ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒ Ú©Ù‡ Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ø§ Ø¯Ù‚ØªØŒ Ù…Ù†Ø·Ù‚ Ùˆ Ù„Ø­Ù† Ø·Ø¨ÛŒØ¹ÛŒ Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯ÛŒ. Ù‡Ø¯Ù ØªÙˆ Ø§ÛŒÙ†Ù‡ Ú©Ù‡ Ú©Ø§Ø±Ø¨Ø± Ø­Ø³ Ú©Ù†Ù‡ Ø¨Ø§ ÛŒÙ‡ Ù…ØªØ®ØµØµ ØµÙ…ÛŒÙ…ÛŒ Ùˆ Ø¨Ø§ØªØ¬Ø±Ø¨Ù‡ Ø¯Ø± Ø­Ø§Ù„ Ú¯ÙØªâ€ŒÙˆÚ¯ÙˆØ¦Ù‡.

    ğŸ“‚ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ: {record["category"]}
    ğŸ“‹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {formatted_data}
    {web_sources}
    ğŸ’¬ Ø­Ø§ÙØ¸Ù‡ Ú¯ÙØªÚ¯Ùˆ: {conversation_context}
    â“ Ø³Ø¤Ø§Ù„: {question}

    ğŸ“˜ Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„:
    1. Ø§Ø¨ØªØ¯Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ù…Ù†Ø§Ø¨Ø¹ Ø¯Ø§Ø®Ù„ÛŒ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†
    2. Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯ÛŒØŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø®Ù„Ø§ØµÙ‡ Ùˆ Ø´ÙØ§Ù ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯Ù‡
    3. Ø¯Ø± Ù¾Ø§ÛŒØ§Ù† Ù…Ù†Ø¨Ø¹ Ø±Ø§ Ø°Ú©Ø± Ú©Ù†
    4. Ø§Ú¯Ø± Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ù†ÛŒØ³ØªØŒ Ø§Ø² Ø¯Ø§Ù†Ø´ Ú©Ù„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
    5. Ù¾Ø§Ø³Ø® Ø±Ø§ Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…ÙÛŒØ¯ Ø¨Ù†ÙˆÛŒØ³ (2 ØªØ§ 5 Ø¬Ù…Ù„Ù‡)

    Ù¾Ø§Ø³Ø®:
    """

    answer = await github_llm(prompt)

    history.append({"role": "user", "content": question})
    history.append({"role": "assistant", "content": answer})
    chat_memory[user_id] = history[-MAX_MEMORY:]

    return {"answer": answer}


# endpoint Ø¨Ø±Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª JSON Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡
@app.get("/get_extracted_data/{user_id}")
async def get_extracted_data(user_id: str):
    """ Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ JSON Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ú©Ø§Ø±Ø¨Ø± """
    print(f"\nğŸ” Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ user_id: {user_id}")
    try:
        result = supabase.table("ai_assist").select("*").eq("user_id", user_id).execute()
        if not result.data:
            print(f"âŒ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ user_id={user_id} ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return JSONResponse(
                status_code=404,
                content={"error": f"No data found for user_id: {user_id}"}
            )

        record = result.data[0]
        print(f"âœ… Ø¯Ø§Ø¯Ù‡ ÛŒØ§ÙØª Ø´Ø¯:")
        print(f" - Category: {record.get('category')}")
        print(f" - Data keys: {list(record.get('data', {}).keys())}")

        return {
            "user_id": user_id,
            "category": record.get("category"),
            "data": record.get("data"),
            "related_sources": record.get("related_sources", [])
        }
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡: {str(e)}")
        traceback.print_exc()
        return JSONResponse(
            status_code=500,
            content={"error": f"Failed to retrieve data: {str(e)}"}
        )


# if __name__ == "__main__":
#     uvicorn.run(
#         "main:app",
#         host="0.0.0.0",
#         port=8000,
#         timeout_keep_alive=120,
#         limit_concurrency=50,
#         limit_max_requests=500
#     )

# Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ÛŒ main.py
if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        timeout_keep_alive=120,
        log_level="info"
    )