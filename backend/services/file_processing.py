import io
import json
import chardet
import tempfile
import os
import traceback
from docx import Document
from langchain_community.document_loaders import PyPDFLoader
from services.text_processing import deep_clean_farsi_text, looks_garbled

async def process_pdf(content: bytes, pages_to_process: int = None) -> dict:
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ PDF Ø¨Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² OCR"""
    with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp_pdf:
        tmp_pdf.write(content)
        pdf_path = tmp_pdf.name

    context = ""
    context_blocks = []
    use_ocr = False

    try:
        # Ù…Ø±Ø­Ù„Ù‡ 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨Ø§ PyPDFLoader
        try:
            reader = PyPDFLoader(pdf_path)
            pages = reader.load()
            
            if pages_to_process and len(pages) > pages_to_process:
                pages = pages[:pages_to_process]
                print(f"âš ï¸ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† ØµÙØ­Ø§Øª Ø¨Ù‡ {pages_to_process} ØµÙØ­Ù‡ Ø§ÙˆÙ„")

            print(f"âœ… ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡: {len(pages)}")
            
            for page_num, page in enumerate(pages, 1):
                page_text = page.page_content
                cleaned_text = deep_clean_farsi_text(page_text)
                if cleaned_text:
                    context += cleaned_text + "\n\n"
                    context_blocks.append({
                        "page": page_num,
                        "text": cleaned_text,
                        "char_count": len(cleaned_text),
                        "method": "pypdfloader"
                    })
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ú©ÛŒÙÛŒØª Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡
            total_chars = len(context.strip())
            print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡: {total_chars}")
            
            if total_chars < 100 or looks_garbled(context):
                print(f"âš ï¸ Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ù†Ø§Ú©Ø§ÙÛŒ ÛŒØ§ Ù†Ø§Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª")
                use_ocr = True
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± PyPDFLoader: {str(e)}")
            use_ocr = True

        # Ù…Ø±Ø­Ù„Ù‡ 2: Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨Ù‡ OCR Ø¨ÙˆØ¯ØŒ Ø§Ø² PyMuPDF Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        if use_ocr:
            print("ğŸ” Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² PyMuPDF Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ†...")
            try:
                import fitz  # PyMuPDF
                
                doc = fitz.open(pdf_path)
                print(f"ğŸ“„ ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª Ø¯Ø± PyMuPDF: {len(doc)}")
                
                # ØªØ¹ÛŒÛŒÙ† Ù…Ø­Ø¯ÙˆØ¯Ù‡ ØµÙØ­Ø§Øª
                if pages_to_process and len(doc) > pages_to_process:
                    page_range = range(min(pages_to_process, len(doc)))
                else:
                    page_range = range(len(doc))
                
                # Ø±ÛŒØ³Øª Ú©Ø±Ø¯Ù† context
                context = ""
                context_blocks = []
                
                for page_num in page_range:
                    page = doc.load_page(page_num)
                    
                    # Ø±ÙˆØ´ 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø³Ø§Ø¯Ù‡
                    text = page.get_text()
                    
                    # Ø±ÙˆØ´ 2: Ø§Ú¯Ø± Ù…ØªÙ† Ú©Ø§ÙÛŒ Ù†Ø¨ÙˆØ¯ØŒ Ø§Ø² Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
                    if not text or len(text.strip()) < 50:
                        text = page.get_text("dict")
                        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² dict
                        blocks_text = []
                        for block in text.get("blocks", []):
                            if block.get("type") == 0:  # Ù†ÙˆØ¹ text
                                for line in block.get("lines", []):
                                    for span in line.get("spans", []):
                                        blocks_text.append(span.get("text", ""))
                        text = " ".join(blocks_text)
                    
                    if text:
                        cleaned_text = deep_clean_farsi_text(text)
                        context += cleaned_text + "\n\n"
                        context_blocks.append({
                            "page": page_num + 1,
                            "text": cleaned_text,
                            "char_count": len(cleaned_text),
                            "method": "pymupdf"
                        })
                
                doc.close()
                print(f"âœ… PyMuPDF: {len(context_blocks)} ØµÙØ­Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯")
                
            except ImportError:
                print("âŒ PyMuPDF Ù†ØµØ¨ Ù†ÛŒØ³Øª! Ù„Ø·ÙØ§ Ø¢Ù† Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯:")
                print("   pip install pymupdf")
                raise Exception("PyMuPDF Ù†ØµØ¨ Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§ Ø¢Ù† Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯.")
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± PyMuPDF: {str(e)}")
                traceback.print_exc()
                raise Exception(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² PDF: {str(e)}")

    finally:
        # Ø­Ø°Ù ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª
        try:
            os.unlink(pdf_path)
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø­Ø°Ù ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª: {str(e)}")

    return {
        "extraction_method": "ocr" if use_ocr else "text",
        "total_characters": len(context),
        "total_blocks": len(context_blocks),
        "full_text": context,
        "blocks": context_blocks,
    }

def process_txt(content: bytes) -> dict:
    detected = chardet.detect(content)
    encoding = detected.get("encoding") or "utf-8"
    raw_text = content.decode(encoding, errors="ignore")
    return {"text": deep_clean_farsi_text(raw_text)}

def process_docx(content: bytes) -> dict:
    doc = Document(io.BytesIO(content))
    full_text = "\n".join([para.text for para in doc.paragraphs])
    return {"text": deep_clean_farsi_text(full_text)}

def process_json(content: bytes) -> dict:
    return json.loads(content.decode("utf-8", errors="ignore"))