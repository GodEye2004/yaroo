import io
import tempfile
import os
import re
import fitz  # PyMuPDF
import pdfplumber
from services.text_processing import deep_clean_farsi_text
import arabic_reshaper
from bidi.algorithm import get_display

# Try to import RapidOCR, handle case if not installed yet
try:
    from rapidocr_onnxruntime import RapidOCR
    ocr_engine = RapidOCR()
    HAS_OCR = True
except ImportError:
    HAS_OCR = False
    print("âš ï¸ RapidOCR not installed. OCR fallback will be disabled.")

def extract_with_pymupdf(pdf_path: str, max_pages: int = None) -> dict:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨Ø§ PyMuPDF - Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ"""
    context = ""
    context_blocks = []
    
    try:
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        
        page_range = range(min(max_pages, total_pages)) if max_pages else range(total_pages)
        
        for page_num in page_range:
            page = doc.load_page(page_num)
            
            # Ø±ÙˆØ´ 1: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§ Ø­ÙØ¸ layout
            text = page.get_text("text", sort=True)
            
            # Ø±ÙˆØ´ 2: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ ØªØ­Ù„ÛŒÙ„ ÙÙˆÙ†Øª
            if not text or len(text.strip()) < 50:
                text_dict = page.get_text("dict")
                blocks_text = []
                
                for block in text_dict.get("blocks", []):
                    if block.get("type") == 0:  # text block
                        block_lines = []
                        
                        for line in block.get("lines", []):
                            line_text = []
                            
                            for span in line.get("spans", []):
                                text_content = span.get("text", "").strip()
                                
                                if text_content:
                                    # ØªØ´Ø®ÛŒØµ Ùˆ Ø§ØµÙ„Ø§Ø­ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
                                    if contains_farsi(text_content):
                                        try:
                                            reshaped = arabic_reshaper.reshape(text_content)
                                            text_content = get_display(reshaped)
                                        except:
                                            pass
                                    
                                    line_text.append(text_content)
                            
                            if line_text:
                                block_lines.append(" ".join(line_text))
                        
                        if block_lines:
                            blocks_text.append("\n".join(block_lines))
                
                text = "\n\n".join(blocks_text)
            
            # Ø±ÙˆØ´ 3: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§ rawdict Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨ÛŒØ´ØªØ±
            if not text or len(text.strip()) < 50:
                raw_dict = page.get_text("rawdict")
                raw_blocks = []
                
                for block in raw_dict.get("blocks", []):
                    if block.get("type") == 0:
                        for line in block.get("lines", []):
                            line_chars = []
                            for span in line.get("spans", []):
                                chars = span.get("chars", [])
                                for char_info in chars:
                                    c = char_info.get("c", "")
                                    if c and c.strip():
                                        line_chars.append(c)
                            
                            if line_chars:
                                line_text = "".join(line_chars)
                                if contains_farsi(line_text):
                                    try:
                                        reshaped = arabic_reshaper.reshape(line_text)
                                        line_text = get_display(reshaped)
                                    except:
                                        pass
                                raw_blocks.append(line_text)
                
                text = "\n".join(raw_blocks)
            
            if text:
                # Ø§ØµÙ„Ø§Ø­ Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ØªÙ†
                text = fix_farsi_text_issues(text)
                text = normalize_farsi_text(text)
                cleaned_text = deep_clean_farsi_text(text)
                
                if cleaned_text and len(cleaned_text.strip()) > 10:
                    context += cleaned_text + "\n\n"
                    context_blocks.append({
                        "page": page_num + 1,
                        "text": cleaned_text,
                        "char_count": len(cleaned_text),
                        "word_count": len(cleaned_text.split()),
                        "method": "pymupdf_advanced"
                    })
        
        doc.close()
        
        return {
            "success": True,
            "text": context,
            "blocks": context_blocks,
            "method": "pymupdf",
            "total_chars": len(context),
            "total_pages": len(context_blocks)
        }
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± PyMuPDF: {str(e)}")
        return {"success": False, "error": str(e)}

def extract_with_pdfplumber(pdf_path: str, max_pages: int = None) -> dict:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§ pdfplumber - Ø¯Ù‚ÛŒÙ‚ Ø¨Ø±Ø§ÛŒ layout"""
    context = ""
    context_blocks = []
    
    try:
        with pdfplumber.open(pdf_path) as pdf:
            total_pages = len(pdf.pages)
            pages_to_process = min(max_pages, total_pages) if max_pages else total_pages
            
            for page_num in range(pages_to_process):
                page = pdf.pages[page_num]
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ø§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
                text = page.extract_text(
                    x_tolerance=2,
                    y_tolerance=2,
                    layout=True,
                    x_density=7.25,
                    y_density=13
                )
                
                # Ø§Ú¯Ø± Ù†ØªÛŒØ¬Ù‡ Ø®ÙˆØ¨ Ù†Ø¨ÙˆØ¯ØŒ Ø§Ø² Ø±ÙˆØ´ Ø¯ÛŒÚ¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
                if not text or len(text.strip()) < 50:
                    text = page.extract_text()
                
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ø¯Ø§ÙˆÙ„ Ù‡Ù… Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª
                tables = page.extract_tables()
                if tables:
                    for table in tables:
                        table_text = "\n".join([" | ".join([str(cell) if cell else "" for cell in row]) for row in table])
                        text += f"\n\n{table_text}"
                
                if text:
                    text = fix_farsi_text_issues(text)
                    text = normalize_farsi_text(text)
                    cleaned_text = deep_clean_farsi_text(text)
                    
                    if cleaned_text and len(cleaned_text.strip()) > 10:
                        context += cleaned_text + "\n\n"
                        context_blocks.append({
                            "page": page_num + 1,
                            "text": cleaned_text,
                            "char_count": len(cleaned_text),
                            "word_count": len(cleaned_text.split()),
                            "method": "pdfplumber"
                        })
        
        return {
            "success": True,
            "text": context,
            "blocks": context_blocks,
            "method": "pdfplumber",
            "total_chars": len(context),
            "total_pages": len(context_blocks)
        }
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± pdfplumber: {str(e)}")
        return {"success": False, "error": str(e)}

def extract_with_ocr(pdf_path: str, max_pages: int = None) -> dict:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø¨Ø§ OCR - Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ú©Ù† Ø´Ø¯Ù‡"""
    if not HAS_OCR:
        return {"success": False, "error": "Library rapidocr-onnxruntime not installed"}
        
    context = ""
    context_blocks = []
    
    try:
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        pages_to_process = min(max_pages, total_pages) if max_pages else total_pages
        
        print(f"ğŸ“· Ø´Ø±ÙˆØ¹ Ù¾Ø±Ø¯Ø§Ø²Ø´ OCR Ø¨Ø±Ø§ÛŒ {pages_to_process} ØµÙØ­Ù‡...")
        
        for page_num in range(pages_to_process):
            page = doc.load_page(page_num)
            
            # ØªØ¨Ø¯ÛŒÙ„ ØµÙØ­Ù‡ Ø¨Ù‡ ØªØµÙˆÛŒØ± Ø¨Ø§ Ú©ÛŒÙÛŒØª Ø¨Ø§Ù„Ø§ (zoom=2)
            mat = fitz.Matrix(2, 2)
            pix = page.get_pixmap(matrix=mat)
            img_bytes = pix.tobytes("png")
            
            # Ø§Ø¬Ø±Ø§ÛŒ OCR
            result, elapse = ocr_engine(img_bytes)
            
            if result:
                # Ù†ØªÛŒØ¬Ù‡ Ù„ÛŒØ³Øª Ø´Ø§Ù…Ù„ [ØªØ®ØªØŒ Ø¬Ø¹Ø¨Ù‡ØŒ Ø§Ù…ØªÛŒØ§Ø²] Ø§Ø³Øª
                page_text = "\n".join([line[1] for line in result])
                
                if page_text:
                    # Ø§ØµÙ„Ø§Ø­ Ùˆ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
                    text = fix_farsi_text_issues(page_text)
                    text = normalize_farsi_text(text)
                    cleaned_text = deep_clean_farsi_text(text)
                    
                    if cleaned_text and len(cleaned_text.strip()) > 10:
                        context += cleaned_text + "\n\n"
                        context_blocks.append({
                            "page": page_num + 1,
                            "text": cleaned_text,
                            "char_count": len(cleaned_text),
                            "word_count": len(cleaned_text.split()),
                            "method": "rapidocr"
                        })
        
        doc.close()
        
        return {
            "success": True,
            "text": context,
            "blocks": context_blocks,
            "method": "rapidocr",
            "total_chars": len(context),
            "total_pages": len(context_blocks)
        }
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± OCR: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

def contains_farsi(text: str) -> bool:
    """Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ù…ØªÙ† Ø´Ø§Ù…Ù„ Ø­Ø±ÙˆÙ ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª"""
    farsi_pattern = re.compile(r'[\u0600-\u06FF\uFB50-\uFDFF\uFE70-\uFEFF]')
    return bool(farsi_pattern.search(text))

def normalize_farsi_text(text: str) -> str:
    """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
    if not text:
        return ""
    
    # 1. ØªØ¨Ø¯ÛŒÙ„ Ø­Ø±ÙˆÙ Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
    arabic_to_farsi = {
        'Ùƒ': 'Ú©', 'ÙŠ': 'ÛŒ', 'Ù‰': 'ÛŒ',
        'Ø©': 'Ù‡', 'Ø¤': 'Ùˆ', 'Ø¥': 'Ø§',
        'Ø£': 'Ø§', 'Ù±': 'Ø§', 'Ø¡': ''
    }
    
    for arabic, farsi in arabic_to_farsi.items():
        text = text.replace(arabic, farsi)
    
    # 2. ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
    arabic_numbers = 'Ù Ù¡Ù¢Ù£Ù¤Ù¥Ù¦Ù§Ù¨Ù©'
    farsi_numbers = 'Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹'
    trans_table = str.maketrans(arabic_numbers, farsi_numbers)
    text = text.translate(trans_table)
    
    # 3. Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ú©Ù†ØªØ±Ù„ Ùˆ Ù†Ø§Ù…Ø±Ø¦ÛŒ
    text = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F-\x9F]', '', text)
    
    # 4. Ø§ØµÙ„Ø§Ø­ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªØ¹Ø¯Ø¯
    text = re.sub(r' +', ' ', text)
    text = re.sub(r'\n\n+', '\n\n', text)
    
    # 5. Ø§ØµÙ„Ø§Ø­ Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡
    text = text.replace('\u200c', ' ')  # Ø­Ø°Ù Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ Ù†Ø§Ù…Ø±Ø¦ÛŒ
    
    return text.strip()

def fix_farsi_text_issues(text: str) -> str:
    """Ø§ØµÙ„Ø§Ø­ Ù…Ø´Ú©Ù„Ø§Øª Ø®Ø§Øµ Ø§Ø³ØªØ®Ø±Ø§Ø¬ PDF ÙØ§Ø±Ø³ÛŒ"""
    if not text:
        return ""
    
    # 1. Ø§ØµÙ„Ø§Ø­ Ú©Ù„Ù…Ø§Øª Ù…Ø¹ÛŒÙˆØ¨ Ø±Ø§ÛŒØ¬
    common_fixes = {
        'Ù‚Ù‚ÛŒ': 'Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯',
        'ØµÛŒ': 'Ø³Ø±Ù…Ø§ÛŒÙ‡',
        'Ù‚ÛŒÛŒ': 'Ø´Ø±Ú©Øª',
        'Ù‡Ø±Ù…ÛŒ': 'Ø³Ù‡Ø§Ù…ÛŒ',
        'Ø® ØµÛŒ': 'Ø®ØµÙˆØµÛŒ',
        'Ù¾ÛŒ': 'Ù¾Ø°ÛŒØ±',
        'Ù…Ø³Ùˆ': 'Ù…Ø³Ø¦ÙˆÙ„',
        'ØªÙ…ÛŒ': 'ØªÙ…Ø§Ù…',
        'Ù‚Ø¹ÙÙ‡': 'Ù‚Ø·Ø¹Ù‡',
        'Ù†Ú¯Ø±': 'Ù†Ú¯Ø§Ø±',
        'Ù‡ÙˆÙ…': 'Ø¹Ù…ÙˆÙ…',
        'Ù…Ø¯ÛŒ': 'Ù…Ø¯ÛŒØ±',
        'Ø¹Ù…Ù„': 'Ø¹Ø§Ù…Ù„',
        'Ø¯Ø§ Ø±ÙˆØ¯ÛŒ': 'Ø¯Ø§ÙˆØ±Ø¯ÛŒ',
    }
    
    for wrong, correct in common_fixes.items():
        text = text.replace(wrong, correct)
    
    # 2. Ø§ØµÙ„Ø§Ø­ Ø´Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙ„ÙÙ† Ù…Ø¹ÛŒÙˆØ¨
    text = re.sub(r'(\d{2,3})\s+(\d{3,4})\s+(\d{4})', r'\1-\2-\3', text)
    
    # 3. Ø§ØµÙ„Ø§Ø­ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ÛŒÙˆØ¨
    text = re.sub(r'(\w+)\s*@\s*(\w+)\s*\.\s*(\w+)', r'\1@\2.\3', text)
    
    # 4. Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø¨ÛŒÙ† Ú©Ù„Ù…Ø§Øª
    text = re.sub(r'([Ø¢-ÛŒ])\s+([Ø¢-ÛŒ])', r'\1\2', text)
    
    # 5. Ø§ØµÙ„Ø§Ø­ Ù†Ù‚Ø·Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ
    text = re.sub(r'\s*\.\s*', '. ', text)
    text = re.sub(r'\s*ØŒ\s*', 'ØŒ ', text)

    # 6. Ø§ØµÙ„Ø§Ø­ Ù¾ÛŒØ´ÙˆÙ†Ø¯Ù‡Ø§ Ùˆ Ù¾Ø³ÙˆÙ†Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯Ø§ Ø§ÙØªØ§Ø¯Ù‡ (Heuristics)
    # Ø§ØªØµØ§Ù„ "Ù…ÛŒ" Ùˆ "Ù†Ù…ÛŒ" Ø¨Ù‡ ÙØ¹Ù„ Ø¨Ø¹Ø¯ÛŒ
    # Note: (?<=^|\s) is invalid in Python because ^ is zero-width and \s is not.
    # We use capturing group (^|\s) instead.
    # IMPORTANT: Replacement string must NOT be raw string if we use \u escape
    text = re.sub(r'(^|\s)(Ù…ÛŒ|Ù†Ù…ÛŒ)\s+(?=[Ø¢-ÛŒ])', '\\1\\2\u200c', text)
    
    # Ø§ØªØµØ§Ù„ "Ù‡Ø§" Ùˆ "Ù‡Ø§ÛŒ" Ø¨Ù‡ Ú©Ù„Ù…Ù‡ Ù‚Ø¨Ù„ÛŒ
    text = re.sub(r'(?<=[Ø¢-ÛŒ])\s+(Ù‡Ø§|Ù‡Ø§ÛŒ)(?=\s|$|\.|ØŒ)', '\u200c\\1', text)
    
    # Ø§ØªØµØ§Ù„ "ØªØ±" Ùˆ "ØªØ±ÛŒÙ†"
    text = re.sub(r'(?<=[Ø¢-ÛŒ])\s+(ØªØ±|ØªØ±ÛŒÙ†)(?=\s|$|\.|ØŒ)', '\u200c\\1', text)
    
    return text

async def process_pdf_advanced(content: bytes, max_pages: int = None) -> dict:
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú†Ù†Ø¯Ù…Ø±Ø­Ù„Ù‡â€ŒØ§ÛŒ PDF Ø¨Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´"""
    
    with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp_pdf:
        tmp_pdf.write(content)
        pdf_path = tmp_pdf.name
    
    results = []
    methods = [
        ("PyMuPDF", extract_with_pymupdf),
        ("PDFPlumber", extract_with_pdfplumber),
    ]
    
    best_result = None
    max_quality_score = 0
    
    for method_name, extractor in methods:
        print(f"ğŸ” ØªØ³Øª Ø±ÙˆØ´ {method_name}...")
        
        try:
            result = extractor(pdf_path, max_pages)
            
            if result["success"]:
                text_length = len(result["text"].strip())
                word_count = len(result["text"].split())
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù…ØªÛŒØ§Ø² Ú©ÛŒÙÛŒØª
                # ÙØ±Ù…ÙˆÙ„: Ø·ÙˆÙ„ Ù…ØªÙ† + Ø§Ù…ØªÛŒØ§Ø² Ú©Ù„Ù…Ø§Øª
                # Ø§Ú¯Ø± Ù…ØªÙ† Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ Ø¨Ø§Ø´Ø¯ØŒ Ø§Ù…ØªÛŒØ§Ø² Ù…Ù†ÙÛŒ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯
                if text_length < 50:
                    quality_score = 0
                else:
                    quality_score = text_length + (word_count * 2)
                
                results.append({
                    "method": method_name,
                    "chars": text_length,
                    "words": word_count,
                    "score": quality_score
                })
                
                print(f"âœ… {method_name}: {text_length} Ú©Ø§Ø±Ø§Ú©ØªØ±ØŒ {word_count} Ú©Ù„Ù…Ù‡ (Ø§Ù…ØªÛŒØ§Ø²: {quality_score})")
                
                if quality_score > max_quality_score:
                    max_quality_score = quality_score
                    best_result = result
            else:
                print(f"âŒ {method_name} Ù†Ø§Ù…ÙˆÙÙ‚: {result.get('error', 'Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡')}")
                
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± {method_name}: {str(e)}")
            
    # Ø§Ú¯Ø± Ù†ØªÛŒØ¬Ù‡ Ø¶Ø¹ÛŒÙ Ø¨ÙˆØ¯ Ùˆ OCR Ø¯Ø§Ø±ÛŒÙ…ØŒ OCR Ø±Ø§ ØªØ³Øª Ú©Ù†
    if (not best_result or max_quality_score < 200) and HAS_OCR:
        print("âš ï¸ Ú©ÛŒÙÛŒØª Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù¾Ø§ÛŒÛŒÙ† Ø¨ÙˆØ¯. ØªÙ„Ø§Ø´ Ø¨Ø§ OCR...")
        try:
            ocr_result = extract_with_ocr(pdf_path, max_pages)
            if ocr_result["success"]:
                text_length = len(ocr_result["text"].strip())
                # OCR Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø§Ø³Øª Ø¨Ø±Ø§ÛŒ Ø§Ø³Ú©Ù†ØŒ Ù¾Ø³ Ø¶Ø±ÛŒØ¨ Ø¨Ø§Ù„Ø§ØªØ±
                quality_score = text_length * 3 
                
                results.append({
                    "method": "RapidOCR",
                    "chars": text_length,
                    "words": len(ocr_result["text"].split()),
                    "score": quality_score
                })
                
                if quality_score > max_quality_score:
                    print(f"âœ… OCR Ù†ØªÛŒØ¬Ù‡ Ø¨Ù‡ØªØ±ÛŒ Ø¯Ø§Ø¯: {text_length} Ú©Ø§Ø±Ø§Ú©ØªØ±")
                    best_result = ocr_result
                else:
                     print(f"â„¹ï¸ OCR Ù‡Ù… Ù†ØªÛŒØ¬Ù‡ Ø¨Ù‡ØªØ±ÛŒ Ù†Ø¯Ø§Ø´Øª ({text_length} Ú©Ø§Ø±Ø§Ú©ØªØ±)")
        except Exception as e:
             print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø¬Ø±Ø§ÛŒ OCR: {e}")
    
    # Ø­Ø°Ù ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª
    try:
        os.unlink(pdf_path)
    except:
        pass
    
    if not best_result:
        raise Exception("âŒ Ù‡ÛŒÚ† Ø±ÙˆØ´ÛŒ Ù†ØªÙˆØ§Ù†Ø³Øª Ù…ØªÙ† Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†Ø¯")
    
    # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©ÛŒÙÛŒØª Ù†Ù‡Ø§ÛŒÛŒ
    total_chars = len(best_result["text"])
    total_words = len(best_result["text"].split())
    
    quality = "Ø¹Ø§Ù„ÛŒ" if total_chars > 1000 else "Ø®ÙˆØ¨" if total_chars > 500 else "Ù…ØªÙˆØ³Ø·" if total_chars > 100 else "Ø¶Ø¹ÛŒÙ"
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´: {best_result['method']}")
    print(f"ğŸ“ Ú©Ù„ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§: {total_chars:,}")
    print(f"ğŸ“ Ú©Ù„ Ú©Ù„Ù…Ø§Øª: {total_words:,}")
    print(f"ğŸ“„ ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª: {len(best_result['blocks'])}")
    print(f"â­ Ú©ÛŒÙÛŒØª: {quality}")
    print(f"{'='*60}\n")
    
    return {
        "extraction_method": best_result["method"],
        "total_characters": total_chars,
        "total_words": total_words,
        "total_blocks": len(best_result["blocks"]),
        "full_text": best_result["text"],
        "blocks": best_result["blocks"],
        "quality": quality,
        "all_methods_tested": results
    }




