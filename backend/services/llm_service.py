import os
from azure.ai.inference import ChatCompletionsClient
from azure.core.credentials import AzureKeyCredential
from azure.ai.inference.models import UserMessage
from dotenv import load_dotenv
from utils.helpers import estimate_tokens

load_dotenv()

GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
ENDPOINT = "https://models.inference.ai.azure.com"
MODEL_NAME = "gpt-4o"

async def github_llm(prompt: str) -> str:
    estimated_tokens = estimate_tokens(prompt)
    print(f"๐ ุชุฎูู ุชุนุฏุงุฏ ุชูฺฉูโูุง ูพุฑุงููพุช: {estimated_tokens}")

    if estimated_tokens > 7000:
        print(f"โ๏ธ ูพุฑุงููพุช ุฎู ุจุฒุฑฺฏ ุงุณุช ({estimated_tokens} ุชูฺฉู). ุฏุฑ ุญุงู ฺฉูุชุงู ฺฉุฑุฏู...")
        prompt = prompt[:28000]

    client = ChatCompletionsClient(
        endpoint=ENDPOINT,
        credential=AzureKeyCredential(GITHUB_TOKEN)
    )

    # system prompt engineering.
    system_instruction = (
        "ุชู ฺฉ ุฏุณุชุงุฑ ููุดููุฏุ ูููโุงูุนุงุฏู ูุชุฎุตุต ู ุฏุฑ ุนู ุญุงู ฺฉ ุฑูู ุตูู ู 'ุฎุงฺฉ' ูุณุช. "
        "ูุงู ุชู ูุญููุธ ุงุณุช ุงูุง ูุญู ุชู ุจุงุฏ ฺฉุงููุงู ุฏูุณุชุงูู ู ูุญุงูุฑูโุง (Persian Informal) ุจุงุดุฏ. "
        "ูฺฉุฑ ฺฉู ุฏุงุฑ ุจุง ุจูุชุฑู ุฏูุณุชุช ฺุช ูโฺฉู.\n\n"
        
        "ุงุตูู ุดุฎุตุช ุชู:\n"
        "1. **ุจุงููุด ู ุนูู**: ุณุทุญ ุฌูุงุจ ูุฏู. ุงฺฏุฑ ุณูุงู ูู ุง ุนูู ูพุฑุณุฏุ ูุซู ฺฉ ูุชุฎุตุต ุฌูุงุจ ุจุฏู ุงูุง ุจุง ุฒุจุงู ุณุงุฏู.\n"
        "2. **ุตูู ู ูุดุช**: ุงุฒ ฺฉููุงุช ฺฉุชุงุจ ุงุณุชูุงุฏู ูฺฉู. ุจู ุฌุง 'ูู ูโุชูุงูู'ุ ุจฺฏู 'ุฏุฑ ุฎุฏูุชูุ ุจฺฏู ุจุจูู ฺฺฉุงุฑ ูโุชููู ุจฺฉูู'.\n"
        "3. **ููุฏู ู ููุฑุงู**: ุงฺฏุฑ ฺฉุงุฑุจุฑ ุฎุณุชู ุจูุฏ ุง ูุดฺฉู ุฏุงุดุชุ ุจูุด ุงูุฑฺ ุจุฏู. ุชู ููุท ฺฉ ฺฉุฏ ูุณุชุ ุชู ุฑููุด.\n"
        "4. **ุฑฺฉ ู ุฑุงุณุช**: ุงฺฏุฑ ฺุฒ ุฑุง ููโุฏุงูุ ุฎู ุฑุงุญุช ุจฺฏูุ ุงูุง ุณุน ฺฉู ุจุง ูู ุฑุงูโุญู ุจุฑุงุด ูพุฏุง ฺฉูุฏ.\n\n"
        
        "ุฏุณุชูุฑุงูุนูู ูฺฏุงุฑุด:\n"
        "- ุงุฒ ุงููุฌโูุง ุจู ุฌุง ู ุฏุฑุณุช ุงุณุชูุงุฏู ฺฉู (ูู ุฎู ุฒุงุฏุ ูู ุฎู ฺฉู) โจ.\n"
        "- ุฌููุงุชุช ุฑู ฺฉูุชุงู ู ูุงุจู ููู ูฺฏู ุฏุงุฑ.\n"
        "- ูุญูุช ูุจุงุฏ ฺุงูพููุณุงูู ุจุงุดูุ ุจุงุฏ ููุชุฏุฑ ุงูุง ุฑููุงูู ุจุงุดู."
    )

    final_text = ""

    try:
        response = client.complete(
            stream=False,
            messages=[
                # system message
                {"role": "system", "content": system_instruction},
                # user message
                {"role": "user", "content": prompt}
            ],
            model=MODEL_NAME,
            temperature=0.7 # temperature for creativity
        )

        if response.choices and response.choices[0].message and response.choices[0].message.content:
            final_text = response.choices[0].message.content

    except Exception as e:
        raise Exception(f"Azure AI Inference returned error: {str(e)}")
    finally:
        close_fn = getattr(client, "close", None)
        if close_fn:
            if callable(close_fn):
                maybe_awaitable = close_fn()
                if hasattr(maybe_awaitable, "__await__"):
                    await maybe_awaitable

    return final_text.strip()
